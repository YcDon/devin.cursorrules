# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- For multi-agent system: Use `o1-preview` for Planner (via OpenAI API) and Claude-3.5 (via Cursor Pro) for Executor - no separate Anthropic API key needed when using Cursor Pro.

## OpenAI Fine-tuning Lessons

### DPO (Direct Preference Optimization) Fine-tuning Setup

1. Data Format:
   - Use JSONL format with each line containing:
     ```json
     {
       "input": {
         "messages": [
           {"role": "system", "content": "system message"},
           {"role": "user", "content": "user message"}
         ]
       },
       "preferred_output": [
         {"role": "assistant", "content": "preferred response"}
       ],
       "non_preferred_output": [
         {"role": "assistant", "content": "non-preferred response"}
       ]
     }
     ```

2. API Parameters:
   - Model: Use `gpt-4o-2024-08-06` (or latest version)
   - Method: Must specify DPO method correctly:
     ```python
     method={
         "type": "dpo"  # This is the correct way to enable DPO
     }
     ```
   - File purpose must be 'fine-tune' when uploading

3. Implementation:
   ```python
   def create_finetune_job(client: OpenAI, training_file_id: str) -> str:
       response = client.fine_tuning.jobs.create(
           training_file=training_file_id,
           model="gpt-4o-2024-08-06",
           method={
               "type": "dpo"
           }
       )
       return response.id
   ```

4. Common Issues Fixed:
   - Don't put DPO settings in `hyperparameters` (deprecated)
   - Don't use `training_method` parameter (not supported)
   - Make sure data format matches DPO requirements exactly
   - File must be uploaded with purpose='fine-tune'

5. Monitoring:
   - Use `client.fine_tuning.jobs.retrieve(job_id)` to check status
   - Use `client.fine_tuning.jobs.list_events(job_id)` for detailed progress
   - Use `client.fine_tuning.jobs.cancel(job_id)` to stop a job
   - Monitor progress in OpenAI console for visual feedback

# Multi-Agent Scratchpad

## Background and Motivation

Project Goal: Fine-tune GPT-4o model programmatically for multiple objectives:
1. Tone preferences (concise, complying, sensational)
2. Tool calling capabilities enhancement
3. Sales conversion optimization through multi-turn conversation handling

Key Requirements:
- Implementation must be programmatic (not using web UI console)
- Need to handle multiple training objectives simultaneously
- Focus on practical implementation and validation
- Must ensure efficient use of API resources given the complexity

The executor has access to three tools: invoking 3rd party LLM, invoking web browser, invoking search engine.

## Key Challenges and Analysis

1. Model Selection and Capabilities
   - Available models for fine-tuning:
     * `gpt-4o-2024-08-06` and `gpt-4o-mini-2024-07-18` (recommended for most users)
     * Both support 128K tokens context length (65,536 for training currently)
   - Can fine-tune a fine-tuned model for iterative improvements
   - Need to consider token limits and cost implications

2. Data Format Requirements
   - Must use chat format with messages array
   - Each example must include role (system, user, assistant)
   - Minimum 10 examples required, recommended 50-100 for clear improvements
   - Support for multi-turn conversations with weight parameter
   - Need to handle token limits (65,536 tokens per training example)

3. Multi-Objective Strategy
   - **Adopt an Integrated Data Approach to Handle Multiple Objectives Simultaneously**
     - Design training examples that incorporate all three objectives within the same conversation
       * Create dialogues where tone preferences, tool calling, and sales conversion tactics are applied together
     - Ensure that the model learns to balance and prioritize multiple directives based on context
   - **Data Structuring and Annotation**
     - Develop an annotation schema to label each aspect of the objectives within the data
       * Use metadata or in-line annotations to indicate desired tone, tool usage, and sales strategies
     - Facilitate easier parsing and adjustment of data for hyperparameter tuning
   - **Leverage Multi-Turn Conversations**
     - Utilize complex, realistic dialogues that mimic real-world interactions
     - Capture scenarios where the assistant must seamlessly switch tones, invoke tools, and employ sales techniques
   - **Progressive Complexity and Curriculum Learning**
     - Start with examples focusing on individual objectives
     - Gradually introduce combined-objective examples to help the model build on learned behaviors
   - **Train/Test Split for Multi-Objective Data**
     - Ensure the split maintains a balanced representation of all objectives
     - Validate that the test set effectively evaluates the model's performance across combined objectives

4. Cost and Resource Management
   - Training cost formula: (base cost per 1M tokens ÷ 1M) × input tokens × epochs
   - Can optimize costs by:
     * Using `gpt-4o-mini` instead of full `gpt-4o`
     * Careful management of training epochs
     * Efficient token usage in examples
   - Need to consider both training and inference costs

## Verifiable Success Criteria

1. Tone Adaptation Success:
   - Model consistently produces output in specified tones (concise/complying/sensational)
   - 90%+ accuracy in blind testing for tone adherence
   - Maintains coherence and context while adapting tone

2. Tool Calling Capabilities:
   - 95%+ accuracy in function calling syntax
   - Correct parameter usage in 90%+ of cases
   - Appropriate tool selection in 85%+ of scenarios

3. Sales Conversion Optimization:
   - Demonstrable improvement in conversion rates (target: +20%)
   - Maintains natural conversation flow
   - Successfully handles common objections
   - Appropriate use of sales techniques

4. Technical Implementation:
   - Zero critical failures in production deployment
   - API usage within allocated budget
   - Response times within acceptable ranges
   - Successful logging and monitoring implementation

5. Data Quality Metrics:
   - Minimum 50 examples per objective
   - Clear train/test split for validation
   - Token count within 65,536 limit per example
   - Consistent formatting across all examples

6. Cost Efficiency:
   - Training cost within budget (calculate using provided formula)
   - Inference cost competitive with base model
   - Optimal use of tokens in training data

## High-level Task Breakdown

Phase 1: Setup and Infrastructure
1. Environment setup and API configuration
2. Development of data processing pipeline
3. Implementation of monitoring and logging

Phase 2: Data Preparation
1. Training data collection for each objective
2. Data cleaning and validation
3. Format standardization
4. Quality assurance checks

Phase 3: Fine-tuning Implementation
1. Initial model training setup
2. Iterative fine-tuning process
3. Progress tracking and model versioning
4. Performance optimization

Phase 4: Testing and Validation
1. Test suite development
2. Comprehensive testing across all objectives
3. Performance metrics collection
4. Refinement based on results

Phase 5: Deployment and Monitoring
1. Production deployment setup
2. Monitoring system implementation
3. Performance tracking
4. Continuous improvement process

## Current Status / Progress Tracking

Phase 1: Setup and Infrastructure
- ✓ Environment setup completed
  * Virtual environment created and activated
  * Core dependencies installed
  * OpenAI API key configured

- ✓ Initial fine-tuning module created (`tools/fine_tune_utils.py`)
  * Dataset management classes implemented
  * Job management functionality implemented
  * Basic evaluation framework in place

Next Steps in Progress:
1. Implement evaluation metrics for each objective:
   - Tone consistency evaluation
   - Tool calling accuracy assessment
   - Sales conversion effectiveness measurement

2. Create data collection and preparation pipeline:
   - Define data formats for each objective
   - Implement data validation rules
   - Set up data preprocessing functions

3. Set up monitoring and logging:
   - Integration with existing token tracking
   - Job status monitoring
   - Cost tracking and optimization

## Next Steps and Action Items

1. Data Preparation (Immediate Priority)
   - **Develop a Unified Dataset that Combines All Objectives**
     - Collect or create conversation examples that simultaneously address tone preferences, tool calling, and sales conversion optimization
     - Aim for at least 50 comprehensive examples that encapsulate multiple objectives
   - **Implement a Robust Annotation Schema**
     - Label each example with metadata for:
       * Desired tone (e.g., concise, complying, sensational)
       * Required tool calls and correct usage
       * Sales strategies employed (e.g., objection handling, closing techniques)
     - This will assist in targeted evaluation and future data adjustments
   - **Format Examples According to OpenAI Chat Completion Format**
     - Ensure each example correctly uses the 'messages' array with roles (system, user, assistant)
     - Incorporate system prompts that set context for multiple objectives
   - **Implement Train/Test Split (80/20)**
     - Split the unified dataset while maintaining the distribution of combined objectives
     - Verify that both sets contain a representative mix of tones, tool usage, and sales strategies
   - **Validate Token Counts and Formatting**
     - Use `tiktoken` to ensure each example is within the 65,536 token limit
     - Perform formatting checks to maintain consistency and adherence to OpenAI's requirements

2. Fine-tuning Pipeline Updates
   - Modify `fine_tune_utils.py` to support:
     * Proper JSONL formatting
     * Token count validation
     * Cost estimation
     * Train/test split functionality

3. Testing Framework
   - Implement evaluation using OpenAI's recommended metrics
   - Set up Weights & Biases integration for tracking
   - Create automated validation pipeline

Next Immediate Tasks for Executor:
1. Update `fine_tune_utils.py` to match OpenAI's requirements:
   - JSONL file format handling
   - Token count validation
   - Cost estimation functions
2. Create data preparation pipeline
3. Set up evaluation framework

## Executor's Feedback or Assistance Requests

Current Status:
1. Implemented comprehensive fine-tuning utilities in `fine_tune_utils.py`:
   - Added token counting using `tiktoken`
   - Implemented cost estimation based on token count and model type
   - Created unified dataset preparation with train/test split
   - Added proper JSONL formatting according to OpenAI's requirements
   - Implemented hyperparameter support
   - Added CLI interface for all operations

2. Key Features Added:
   - Token counting and validation (65,536 token limit per example)
   - Cost estimation based on model type (gpt-4o-mini vs full gpt-4o)
   - Train/test dataset split with proper formatting
   - Comprehensive metrics tracking
   - Support for custom system prompts
   - CLI interface for prepare/train/status/evaluate operations

3. Implementation Details:
   - Using `tiktoken` for accurate token counting
   - Supporting both gpt-4o-mini and full gpt-4o models
   - Proper JSONL formatting with messages array
   - Integration with token_tracker for usage monitoring
   - Automated cost estimation based on token count and epochs

Current Blockers:
1. Need to verify token counting accuracy with actual OpenAI API
2. Need to confirm current pricing for fine-tuning (used placeholder values)
3. Need to test the hyperparameter configuration options
4. Need to verify the metrics format matches OpenAI's actual response

Requested Actions from Planner:
1. Review the token counting implementation and confirm if it matches OpenAI's counting method
2. Provide guidance on optimal hyperparameter settings for each objective
3. Confirm if the current metrics structure matches OpenAI's actual response format
4. Advise on best practices for system prompts for each objective (tone, tool_calling, sales)
5. Suggest appropriate train/test split ratio for different dataset sizes